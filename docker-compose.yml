---
services:
  weaviate:
    command:
    - --host
    - 0.0.0.0
    - --port
    - '8080'
    - --scheme
    - http
    image: cr.weaviate.io/semitechnologies/weaviate:1.34.0
    ports:
    - 8080:8080
    - 50051:50051
    volumes:
    - weaviate_data:/var/lib/weaviate
    restart: on-failure:0
    environment:
      QUERY_DEFAULTS_LIMIT: 25
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'
      PERSISTENCE_DATA_PATH: '/var/lib/weaviate'
      ENABLE_MODULES: 'text2vec-transformers,multi2vec-clip,text2vec-ollama,text2vec-cohere,reranker-transformers,generative-ollama'
      CLUSTER_HOSTNAME: 'node1'
      TRANSFORMERS_INFERENCE_API: http://text2vec-transformers:8080
      CLIP_INFERENCE_API: http://multi2vec-clip:8080
      RERANKER_INFERENCE_API: http://reranker-transformers:8080
      OLLAMA_API_ENDPOINT: http://ollama:11434
      # COHERE_APIKEY: 'your-cohere-api-key-here'  # Cohere Vision使用時のみ設定
    depends_on:
      - ollama

  text2vec-transformers:
    image: cr.weaviate.io/semitechnologies/transformers-inference:sentence-transformers-multi-qa-MiniLM-L6-cos-v1
    environment:
      ENABLE_CUDA: '1'
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  reranker-transformers:
    image: cr.weaviate.io/semitechnologies/reranker-transformers:cross-encoder-ms-marco-MiniLM-L-6-v2
    environment:
      ENABLE_CUDA: '1'
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  ollama:
    image: ollama/ollama:latest
    ports:
    - 11434:11434
    volumes:
    - ollama_data:/root/.ollama

  multi2vec-clip:
    image: cr.weaviate.io/semitechnologies/multi2vec-clip:sentence-transformers-clip-ViT-B-32-multilingual-v1
    environment:
      ENABLE_CUDA: '1'
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  # HuggingFace Transformers用のマルチモーダルモデルサービス（Qwen-VLなど）
  multi2vec-transformers-qwen:
    image: cr.weaviate.io/semitechnologies/transformers-inference:Qwen/Qwen2.5VL-7B-Instruct
    # 注意: 実際のモデル名に応じて調整が必要です
    # 利用可能なマルチモーダルモデルを指定してください
    ports:
      - 8081:8080  # デフォルトのCLIPサービスと競合しないように別ポート
    environment:
      ENABLE_CUDA: '1'
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

volumes:
  weaviate_data:
  ollama_data:
