<!-- 996c604f-acd3-4831-b33b-7296b8da0eb9 e97c0f52-af30-48ff-8360-d2190f481d65 -->
# マルチモーダルベクトルデータベース構築計画（HuggingFace Transformers中心）

## 概要

Weaviateを使用して、画像とテキストを同一ベクトル空間で扱えるマルチモーダルなベクトルストアを構築します。**HuggingFaceのTransformersを中心に**、複数のEmbeddingモデルを試せるスキーマ構成と、ローカルLLM（Ollama）を使用したRAG機能を実装します。

## 重要な技術的理解

### Weaviateのマルチモーダルベクトル化の仕組み

1. **multi2vec-clipモジュール**: Weaviateのマルチモーダルベクトル化モジュール

   - デフォルトでCLIPモデル専用サービスを使用可能
   - `inference_url`パラメータでカスタムの`transformers-inference`サービスを指定可能
   - HuggingFaceのマルチモーダルモデル（Qwen-VLなど）を使用する場合、専用の`transformers-inference`サービスを作成し、そのURLを`inference_url`で指定

2. **transformers-inferenceサービス**: HuggingFaceモデルを実行するサービス

   - イメージタグでモデルを指定（例：`transformers-inference:Qwen/Qwen2-VL-2B-Instruct`）
   - マルチモーダルモデルもサポート可能
   - 各モデル用に別々のサービスを作成し、異なるポートで起動

3. **text2vec-cohereモジュール**: Cohere Vision用（外部API）

## 実装内容

### 1. Docker Compose設定の更新

既存の`docker-compose.yml`を以下のように更新：

**追加するサービス**:

- **multi2vec-transformers-qwen**: Qwen-VLなどのマルチモーダルモデル用
  - イメージ: `cr.weaviate.io/semitechnologies/transformers-inference:Qwen/Qwen2-VL-2B-Instruct`（または適切なモデル名）
  - ポート: `8081:8080`（デフォルトのCLIPサービスと競合しないように）
  - 環境変数: `ENABLE_CUDA: '1'`（GPU使用時）

**更新する設定**:

- `ENABLE_MODULES`に`text2vec-cohere`を追加（Cohere Vision使用時）
- `COHERE_APIKEY`環境変数を追加（Cohere Vision使用時）

### 2. マルチモーダルコレクションの作成

各モデルごとにコレクションを作成：

- **MultimodalData_CLIP_ViT_B_32**: 
  - ベクトライザー: `multi2vec-clip`（デフォルトのCLIPサービス使用）
  - ローカル完結

- **MultimodalData_Qwen_VL**: 
  - ベクトライザー: `multi2vec-clip`（`inference_url`でカスタムtransformers-inferenceサービスを指定）
  - ローカル完結、HuggingFaceからモデル取得

- **MultimodalData_Cohere_Vision**: 
  - ベクトライザー: `text2vec-cohere`（`embed-v4.0`モデル）
  - 外部API使用（APIキー必要）

### 3. サンプルコードの作成

#### 3.1 スキーマ作成スクリプト (`sample_code/create_multimodal_schema.py`)

- CLIP用コレクション作成（デフォルトサービス使用）
- Qwen-VL用コレクション作成（`inference_url`でカスタムサービス指定）
- Cohere Vision用コレクション作成
- 各コレクションに`image`（blob型）と`text`（string型）フィールドを定義

#### 3.2 データインポートスクリプト (`sample_code/import_multimodal_data.py`)

- 画像ファイルをbase64エンコードしてインポート
- テキストデータと画像データを同時に格納
- 複数のコレクションに同じデータをインポートしてモデル比較可能に

#### 3.3 マルチモーダル検索スクリプト (`sample_code/multimodal_search.py`)

- テキストクエリによる画像検索（`near_text`）
- 画像クエリによるテキスト検索（`near_image`）
- テキストクエリによるテキスト検索
- 画像クエリによる画像検索

#### 3.4 マルチモーダルRAGスクリプト (`sample_code/multimodal_rag.py`)

- マルチモーダル検索結果を取得
- Ollamaを使用した回答生成（`generative-ollama`モジュール）
- 検索結果のコンテキストを含めた回答生成

### 4. ドキュメントの作成

- `MULTIMODAL_SETUP.md`: セットアップ手順と使用方法
- HuggingFace Transformersモデルの追加方法
- 各スクリプトの使用方法とパラメータ説明

## 技術的な詳細

### ベクトル化モデル

1. **multi2vec-clip（CLIPモデル）**: 

   - デフォルトの`multi2vec-clip`サービスを使用
   - ローカル完結

2. **multi2vec-clip（HuggingFace Transformers）**: 

   - `multi2vec-clip`モジュールを使用
   - カスタム`transformers-inference`サービスを`inference_url`で指定
   - Qwen-VL、Qwen2-VLなどのマルチモーダルモデルを使用可能
   - ローカル完結

3. **text2vec-cohere（Cohere Vision）**: 

   - `embed-v4.0`モデルがマルチモーダル対応
   - 外部API使用

### データ形式

- 画像: base64エンコードされた文字列として`blob`型で格納
- テキスト: `string`型で格納

### RAG実装

- 検索: Weaviateの`near_text`または`near_image`を使用
- 生成: Ollamaの`llama3.2`などのローカルLLMを使用（`generative-ollama`モジュール）

## ファイル構成

```
.
├── docker-compose.yml (更新: HuggingFace Transformersサービス追加)
├── sample_code/
│   ├── create_multimodal_schema.py (新規)
│   ├── import_multimodal_data.py (新規)
│   ├── multimodal_search.py (新規)
│   └── multimodal_rag.py (新規)
└── MULTIMODAL_SETUP.md (新規)
```

## 実装のポイント

1. **HuggingFace Transformers中心**: マルチモーダルベクトル化にHuggingFaceのTransformersを優先的に使用
2. **multi2vec-clipモジュールの活用**: `inference_url`でカスタムサービスを指定することで、様々なHuggingFaceモデルを使用可能
3. **複数モデルの比較**: 各モデル用に別々のコレクションを作成し、同じデータで異なるモデルの性能を比較可能
4. **ローカル完結**: HuggingFaceモデルとCLIPモデルはローカルで動作（Cohere Visionは外部API）
5. **柔軟なスキーマ**: 新しいモデルを追加する際は、新しい`transformers-inference`サービスとコレクションを作成するだけで対応可能

### To-dos

- [ ] docker-compose.ymlの設定を確認し、multi2vec-clipの設定が適切か確認する
- [ ] 複数のEmbeddingモデル用コレクションを作成するスクリプトを作成する
- [ ] 画像とテキストをインポートするスクリプトを作成する（base64エンコード対応）
- [ ] マルチモーダル検索（テキスト→画像、画像→テキスト等）を実装するスクリプトを作成する
- [ ] Ollamaを使用したマルチモーダルRAGスクリプトを作成する
- [ ] セットアップ手順と使用方法を記載したドキュメントを作成する